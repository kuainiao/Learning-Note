## 常用的算法
### SVM和LR的对比
相同点：
1. SVM和LR一般用来处理线性二分类问题，特殊处理之后可以处理多分类问题
2. 两者在使用核函数后，可以处理非线性问题
3. 两者都是判别模型

不同点：
1. LR是参数模型，它假设数据集满足伯努利分布，SVM是非参数模型，它对数据集的分布没有假设。
2. LR使用log loss，SVM使用hinge loss，这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。SVM只与部分样本(支持向量)有关，LR与所有样本有关
3. LR是经验风险最小化，SVM是结构风险最小化
4. 两个模型对数据和参数的敏感程度不同，Linear SVM比较依赖penalty的系数和数据表达空间的测度，而（带正则项的）LR比较依赖对参数做L1 regularization的系数。但是由于他们或多或少都是线性分类器，所以实际上对低维度数据overfitting的能力都比较有限，相比之下对高维度数据，LR的表现会更加稳定，为什么呢？因为Linear SVM在计算margin有多“宽”的时候是依赖数据表达上的距离测度的，换句话说如果这个测度不好（badly scaled，这种情况在高维数据尤为显著），所求得的所谓Large margin就没有意义了，这个问题即使换用kernel trick（比如用Gaussian kernel）也无法完全避免。所以使用Linear SVM之前一般都需要先对数据做normalization，而求解LR（without regularization）时则不需要或者结果不敏感。
5. LR可以给出类别的概率，SVM是非概率的

总结一下：
1. Linear SVM和LR都是线性分类器
2. Linear SVM不直接依赖数据分布，分类平面不受一类点影响；LR则受所有数据点的影响，如果数据不同类别strongly unbalance一般需要先对数据做balancing。
3. Linear SVM依赖数据表达的距离测度，所以需要对数据先做normalization；LR不受其影响
4. Linear SVM依赖penalty的系数，实验中需要做validation 
5. Linear SVM和LR的performance都会收到outlier的影响，其敏感程度而言，谁更好很难下明确结论。

模型选择：
1. 如果Feature的数量很大，跟样本数量差不多，这时候选用LR或者是Linear Kernel的SVM
2. 如果Feature的数量比较小，样本数量一般，不算大也不算小，选用SVM+Gaussian Kernel
3. 如果Feature的数量比较小，而样本数量很多，需要手工添加一些feature变成第一种情况


### 树模型
ID3，C4.5, CART
1. ID3以信息增益为准则来选择最优划分属性(信息增益的计算要基于信息熵，计算分割后与分割前的信息熵的差值，也就是信息增益)；**缺陷：** ID3决策树偏向于取值较多的属性进行分割，存在一定的偏好
2. C4.5基于信息增益率准则选择最优分割属性， 信息增益比率通过引入一个被称作分裂信息(Split information)的项来惩罚取值较多的属性，如果简单的按照这个规则来分割，模型又会偏向特征数少的特征。**因此C4.5决策树先从候选划分属性中找出信息增益高于平均水平的属性，在从中选择增益率最高的。**
3. CART以基尼系数为准则选择最优划分属性，可以应用于分类和回归，CART是一棵二叉树，采用二元切分法，CART分类时，使用基尼指数（Gini）来选择最好的数据分割的特征，gini描述的是纯度，与信息熵的含义相似。

